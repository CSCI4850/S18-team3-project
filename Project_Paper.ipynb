{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style:\"text-align: center;\">\n",
    "<h1>Rich Word Embedding for Text Generation <br/>\n",
    "Using Recurrent Neural Networks</h1>\n",
    "\n",
    "<br/>\n",
    "Samuel Remedios<sup>1,2</sup>, Michael Schmidt<sup>1</sup>, Madison Karrh<sup>1</sup>, Brent Perez<sup>1</sup>, John Westbrooks<sup>1</sup>\n",
    "<br/>\n",
    "\n",
    "<sup>1</sup> Department of Computer Science, Middle Tennessee State University\n",
    "<br/>\n",
    "<sup>2</sup> Center for Neuroscience and Regenerative Medicine, Henry M. Jackson Foundation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract (CURRENTLY JUST OUR PROJECT PROPOSAL)\n",
    "---\n",
    "Language modeling and text generation have been studied for decades, but more recently attempts to use deep learning and other machine learning techniques have been applied to these tasks. The field of natural language processing (NLP) concerns itself with the study and modelling of language such that a computer can interpret and act upon non-curated language as humans use it. There exist a few subfields within NLP, such as text generation, semantic modelling, and sentiment analysis. Currently, the more popular generative methods aim to model languages from the character-level upwards rather than from the word-level. Even those that operate on a word-by-word basis (Markov chain modeling, n-gram models) usually do not incorporate parts-of-speech nor long-term memory. Traditionally, these word-level models build internal representations of language by building a probabilistic mapping between previous words in the sequence to the current word. However, language has more depth than the surface-level words present, such as semantics, grammar, parts of speech, etc, and we feel that it would be advanageous to encode these informations alongside the words themselves.\n",
    "\n",
    "For this project, we propose a method of tokenizing words alongside their parts-of-speech with unique identifiers for use in a recurrent neural network (RNN). Specifically, we aim to generate text using the long short-term memory (LSTM) variant of the RNN using training data comprised of scripts from popular American cartoon television shows, such as the Simpsons, Family Guy, Rick and Morty, South Park, and others. We narrowed the genre of script to be somewhat specific to see if we can model the general flow and cadence of comedy shoows, which should have more related elements in storytelling and dialogue compared to a mix of genres such as comedy, drama, action, and so on.\n",
    "\n",
    "To validate our model, we will first qualitatively inspect the generated scripts. We will then quantitively measure the Levenshtein distance (or other similar metric) between the tokens and parts-of-speech of our training corpus and the generated scripts. In particular, we will generate around some fixed number of sequential words (500-1000), and verify that these generated words' parts of speech are within some similarity range to other subsets of real text compared to other subsets of real text. For example, comparing some fixed-size subsets of Family Guy data to Simpsons data will give us understanding as to how much the Levenshtein part of speech distance can vary among real datasets. We then will use this metric to validate our generative model. Additionally, we are contemplating metrics such as BLAST which compare genetic data, and will utilize them if they are applicable to this type of sequence data. If the metric(s) is/are sensible, we will then compare our methods to already-existing text generation techniques such as character-level RNNs and Markov chain models to judge our model's efficacy.\n",
    "\n",
    "The ability to automatically generate text is two-fold. First, if the results are reasonable, then it may be advantageous to further investigate how the language was modeled using this technique. Second, the automatic generation of stories and scripts could potentially be used as a starting point for writers, or smooth the creation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "---\n",
    "Semantic understanding of natural language has been a difficult problem in computer science for decades. [cite 1] While many great strides have been made in many subfields such as sentiment analysis [cite 2], word embeddings [cite 3], text generation [cite 4], classification [cite5], and summarization [cite 6], there still lacks an effective means of human-level text modelling [cite 7, something from 2016 or 2017].  Recently, more tools have become available to help in this endeavour [cite NLTK, cite SpaCy, cite word2vec], and the aggregation of techniques is now possible.  One such example is the generation of a caption given an image [cite Karpathy's thesis], which uses a convolutional neural network in tandem with a recurrent neural network (RNN), trained on natural language data parsed and tokenized with {NLTK or spacy, some kind of library. Read paper to find out}.\n",
    "\n",
    "{text generation and its importance in text modelling}\n",
    "\n",
    "{example and walkthrough of markov chain modelling}\n",
    "\n",
    "{example and walkthrough of n-gram modelling}\n",
    "\n",
    "{example and walkthrough of RNN character modelling}\n",
    "\n",
    "{citation and example of more modern techniques with word2vec}\n",
    "\n",
    "{introduce our method, combining word2vec embeddings with embedded parts of speech}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and Discussion\n",
    "---\n",
    "{ future work: explore other ways of rich embeddings with more information such as subject/participle, \"arc\" of story, more context, etc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References (will need to figure out how to link to Zotero later, for now web links)\n",
    "---\n",
    "<div class=\"cite2c-biblio\"></div>\n",
    "1. https://www.cl.cam.ac.uk/archive/ksj21/histdw4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
