{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Comedy Animation Scripts Using Recurrent Neural Networks\n",
    "### Madison Karrh, Brent Perez, Samuel Remedios, Michael Schmidt, John Westbrooks\n",
    "### Team Pandemonium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling and text generation have been studied for decades, but more recently attempts to use deep learning and other machine learning techniques have been applied to these tasks.  However, more popular methods aim to model languages from the character-level upwards rather than from the word-level.  Even those that operate on a word-by-word basis (Markov chain modeling, n-gram models) usually do not incorporate parts-of-speech nor long-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we propose a method of tokenizing words alongside their parts-of-speech with unique identifiers for use in a recurrent neural network (RNN).  Specifically, we aim to generate text using the long short-term memory (LSTM) variant of the RNN using training data comprised of scripts from popular American cartoon television shows, such as the Simpsons, Family Guy, Rick and Morty, South Park, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our model, we will first qualitatively inspect the generated scripts. We will then quantitively measure the Levenshtein distance (or other similar metric) between the tokens and parts-of-speech of our training corpus and the generated scripts. In particular, we will generate around some fixed number of sequential words (500-1000), and verify that these generated words' parts of speech are within some similarity range to other subsets of real text compared to other subsets of real text. For example, comparing some fixed-size subsets of Family Guy data to Simpsons data will give us understanding as to how much the Levenshtein part of speech distance can vary among real datasets. We then will use this metric to validate our generative model. Additionally, we are contemplating metrics such as BLAST which compare genetic data, and will utilize them if they are applicable to this type of sequence data. If the metric(s) is/are sensible, we will then compare our methods to already-existing text generation techniques such as character-level RNNs and Markov chain models to judge our model's efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to automatically generate text is two-fold.  First, if the results are reasonable, then it may be advantageous to further investigate how the language was modeled using this technique.  Second, the automatic generation of stories and scripts could potentially be used as a starting point for writers, or smooth the creation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
