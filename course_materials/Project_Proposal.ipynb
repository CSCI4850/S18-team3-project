{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Comedy Animation Scripts Using Recurrent Neural Networks\n",
    "### Madison Karrh, Brent Perez, Samuel Remedios, Michael Schmidt, John Westbrooks\n",
    "### Team Pandemonium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling and text generation have been studied for decades, but more recently attempts to use deep learning and other machine learning techniques have been applied to these tasks. The field of natural language processing (NLP) concerns itself with the study and modelling of language such that a computer can interpret and act upon non-curated language as humans use it. There exist a few subfields within NLP, such as text generation, semantic modelling, and sentiment analysis.  Currently, the more popular generative methods aim to model languages from the character-level upwards rather than from the word-level.  Even those that operate on a word-by-word basis (Markov chain modeling, n-gram models) usually do not incorporate parts-of-speech nor long-term memory.  Traditionally, these word-level models build internal representations of language by building a probabilistic mapping between previous words in the sequence to the current word.  However, language has more depth than the surface-level words present, such as semantics, grammar, parts of speech, etc, and we feel that it would be advanageous to encode these informations alongside the words themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we propose a method of tokenizing words alongside their parts-of-speech with unique identifiers for use in a recurrent neural network (RNN).  Specifically, we aim to generate text using the long short-term memory (LSTM) variant of the RNN using training data comprised of scripts from popular American cartoon television shows, such as the Simpsons, Family Guy, Rick and Morty, South Park, and others. We narrowed the genre of script to be somewhat specific to see if we can model the general flow and cadence of comedy shoows, which should have more related elements in storytelling and dialogue compared to a mix of genres such as comedy, drama, action, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our model, we will first qualitatively inspect the generated scripts. We will then quantitively measure the Levenshtein distance (or other similar metric) between the tokens and parts-of-speech of our training corpus and the generated scripts. In particular, we will generate around some fixed number of sequential words (500-1000), and verify that these generated words' parts of speech are within some similarity range to other subsets of real text compared to other subsets of real text. For example, comparing some fixed-size subsets of Family Guy data to Simpsons data will give us understanding as to how much the Levenshtein part of speech distance can vary among real datasets. We then will use this metric to validate our generative model. Additionally, we are contemplating metrics such as BLAST which compare genetic data, and will utilize them if they are applicable to this type of sequence data. If the metric(s) is/are sensible, we will then compare our methods to already-existing text generation techniques such as character-level RNNs and Markov chain models to judge our model's efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to automatically generate text is two-fold.  First, if the results are reasonable, then it may be advantageous to further investigate how the language was modeled using this technique.  Second, the automatic generation of stories and scripts could potentially be used as a starting point for writers, or smooth the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
