{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March 15 Milestone\n",
    "### Madison Karrh, Brent Perez, Samuel Remedios, Michael Schmidt, John Westbrooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th><p>*Deliverable*</p></th>\n",
    "<th><p>*Percent Complete*</p></th>\n",
    "<th><p>*Estimated Completion Date*</p></th>\n",
    "<th><p>*Percent Complete by March 29th*</p></th>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td><p>Code</p></td>\n",
    "<td><p>$20$%</p></td>\n",
    "<td><p>April $3$rd</p></td>\n",
    "<td><p>$80$%</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><p>Paper</p>\n",
    "</td><td><p>$5$%</p></td>\n",
    "<td><p>April $10$th</p></td>\n",
    "<td><p>$20$%</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><p>Demo</p>\n",
    "</td><td><p>$0$%</p></td>\n",
    "<td><p>April $19$th</p></td>\n",
    "<td><p>$5$%</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><p>Presentation</p>\n",
    "</td><td><p>$0$%</p></td>\n",
    "<td><p>April $19$th</p></td>\n",
    "<td><p>$5$%</p></td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. While no milestones were officially set prior to this one, we've thus far focused on data collection and string parsing. A script has been written to crawl specific transcript websites and pull the transcripts into a file. \n",
    "\n",
    "    To get the most mileage from our ideal RNN, we must transform words into dense vectors.  To accomplish this, we're using the word2vec paradigm[1]. Our current progress allows us to calculate the appropriate, custom word vectors for our specific dataset.\n",
    "\n",
    "    Additionally, since our aim is to enhance text generation with more dimensions, we've made use of NLTK[2] and its averaged perceptron part of speech tagger.  This, in conjunction with Keras' quick transformations of categorical information to one-hot vectors, allows us to pair words with their parts of speech.\n",
    "\n",
    "    Regarding metrics, we've decided to use a Markov transition matrix in addition to the previously-proposed Levenshtein distance for part of speech.  This will allow us to calculate the likelihood of some sequence of words based on the training data.  We will then have two types of quantitative metrics: one measuring structure and one measuring content.\n",
    "\n",
    "    We predict that $90$% of the codebase will parse and process the data, and the remaining $10$% will be the neural network code itself, and as such we're giving proper attention to the data processing code.\n",
    "\n",
    "2. Since no goals were previously set, no goals have not been accomplished to their anticipated percentage.\n",
    "\n",
    "3. For the next milestone on March $29$th, most of our text processing code shoud be done.  \n",
    "\n",
    "    Mike and Madison will work on cleansing the data as well as the general usability of word2vec.  The strings passed into the word2vec transformer must receive words which don't contain typos or are frequent enough.  Punctuation may be handled automatically by NLTK's tokenizer.  Special \"START\" and \"END\" tokens must prepend and append all sentences to aid in the RNN's learning of structure.\n",
    "\n",
    "    Brent and John will focus on RNN design theory and implementation in Keras.  There are some \"best practices\" which exist for neural models, and knowing these would be beneficial in model convergence down the road.  Having example code running on a smaller example will be a great launching pad as well.  Thus, the goal for the next milestone is to have a working Keras implementation of a character-level RNN so that we can have an understanding of how to feed data into an RNN and how to sample from its latent space.\n",
    "\n",
    "    Sam will continue work on the codes to enable quantification of results.  By the next milestone, we should be able to compare the Levenshtein distance between parts of speech of words as well as the likelihood of a sequence occuring, both as scalar values.  If neither of these metrics are sensible, then others will have to be found and implemented by March $29$th.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Jurafsky and J. H. Martin, Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition. India: Dorling Kindersley Pvt, Ltd., 2014.\n",
    "\n",
    "[2] “Source code for nltk.tag.perceptron,” nltk.tag.perceptron - NLTK 3.2.5 documentation. [Online]. Available: http://www.nltk.org/_modules/nltk/tag/perceptron.html. [Accessed: 16-Mar-2018]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
